% Demonstration 1
% Jon Craton
% June 20, 2018

> Provide a document that demonstrates your knowledge of some basic
algorithm analysis. The document can be a word document, powerpoint
slides, a video presentation, …. The format isn’t important and it
doesn’t need to be formal.

# 1.

> Take a fragment of code or pseudocode, and demonstrate you ability
to do “frequency counting” of the statements/operations/blocks of
operations in the code. (As in the example on page 181 of Algs4.)
The code or algorithm you choose is whatever you would like in
whatever language/pseudolanguage you wouldlike, so long as it is
sufficiently complex to effectively demonstrate frequency counting.
In the course of this, you might demonstrate use of a summation
formula and/or induction if appropriate.

We'll demonstrate frequency counting using a simple selection sort[1]. This is one of the simplest possible sorts to implement and has polynomial complexity O(n²).

Here's the implementation in Python:

```python
def selection_sort(items, cmp):
  """ Takes a list of elements and sorts it in place using the simple selection sort algorithm (https://en.wikipedia.org/wiki/Selection_sort)
  
  >>> selection_sort([1,2,3], lambda a,b: a<b)
  [1, 2, 3]
  >>> selection_sort([3,2,1], lambda a,b: a<b)
  [1, 2, 3]
  >>> selection_sort([3,3,4,5,2,1], lambda a,b: a<b)
  [1, 2, 3, 3, 4, 5]
  """
  sorted = []

  while items:
    best = items[0]
  
    for item in items[1:]:
      if cmp(item, best):
        best = item

    items.remove(best)
    sorted.append(best)

  return sorted
```

Notice that this function takes a comparison function `cmp` as an argument. This allows us to use it to sort any items that we can write a comparison function for, but it also allows us to literally count the number of comparisons as the program runs.

Here's an example `less_than` function that implements automatic frequency counting:

```python
def less_than(a, b):
  """ Returns true if a is less than b """

  less_than.count += 1
  return a < b
```

And a shortcut frequency counting function that also measures real execution time:

```python
import time

def freq_count(items, alg, cmp):
  """ Returns number of calls to `cmp` when running `alg` on `items` """
  
  length = len(items)
  start = time.process_time()
  less_than.count = 0
  alg(items, cmp)
  return((less_than.count,time.process_time() - start,length))

freq_count([3,3,4,5,2,1], selection_sort, less_than)
```

We can extend this to create counts for random lists of different sizes:

```python
from random import getrandbits

results = []

for size in range(1, 81):
  results.append(freq_count(list([getrandbits(16) for i in range(0,size)]), selection_sort, less_than))
  print("%d call%s for list size %d" % (results[-1][0], 's' if results[-1][0] > 1 else '', size))
```

Interestingly, these results are the triangle numbers from Pascal's Triangle[2].

Let's also graph these results to inspect them visually:

```python, results='hidden'
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

plt.plot([r[2] for r in results], [r[0] for r in results])
plt.xlabel('List size')
plt.ylabel('Comparisons')
plt.title('Impact of list size')
```

# 2.  

>Take a fragment of code or pseudocode, and demonstrate you ability
to do determine an appropriate cost model (aka performance model)
for the code fragment. Explain the assumptions you made as to why
your choice of performance model was appropriate. The code or
algorithm you choose is whatever you would like, so that is
sufficient to demonstrate frequency counting. It would probably be
convenient to use the same code fragment you chose for problem 1.

Here are the operations we could resonably consider for a cost model:

1. Moving items

In some cases, the number of move operations or swaps could be used as the cost model. Because every item is moved into the list of sorted items exactly once, this is a simple O(n) operation. It doesn't make sense to use this as a cost model because it will be eclipsed in terms of contribution to running time by comparisons which are O(n²)

2. Comparing items

The number of comparisons is frequently used as the cost model for sorting. This is generally the operation that must be repeated the most, and this is true for selection sort where comparisons are O(n²).

3. Loading items

We could also use loading an item as our cost model. Because we are comparing a list item against a local variable, each comparison requires one load, so loads are also O(n²).

The most complete cost model for this case is likely "load and compare", but it is safe to just consider the number of compares as loads are always coupled with compare operations.

## Uniform cost?

In most cases, it would likely be fine to assume a uniform cost model for this algorithm. However, comparison is actually not always a uniform operation, so it may be prudent to use a logarithmic cost model in some cases. Consider the following operation:

```python3
results = []

for size in range(1, 26):
  (_, elapsed, _) = freq_count(list([getrandbits(2**size) for i in range(0,256)]), selection_sort, less_than)
  results.append((size, elapsed))
  print("Sorted 256 integers of size %s in %s seconds" % (2**size, elapsed))
```

Let's graph that output to show actual runtime vs integer size for large integers:

```python, results='hidden'
plt.plot([r[0] for r in results], [r[1] for r in results])
plt.xlabel('Integer size (2ⁿ bits)')
plt.ylabel('Runtime (s)')
plt.title('Runtime impact of int size')
```

This makes it clear that for some integer size (around 2²⁰ bits) the actual cost of doing a comparison between two integers becomes a very significant cost and increases exponentially with the size of the integer.

This isn't the worst we can do, though. Comparisons between big integers are actually quite quick if the integers are very different. Consider the following example:

```python3
results = []

for size in range(4, 17):
  start = getrandbits(2**size)
  (_, elapsed, _) = freq_count(list([start + getrandbits(8) for i in range(0,512)]), selection_sort, less_than)
  (_, elapsed, _) = freq_count(list([start + getrandbits(8) for i in range(0,512)]), selection_sort, less_than)
  results.append((size, elapsed))
  print("Sorted 256 integers of size %s in %s seconds" % (2**size, elapsed))
```

Let's graph that output to show actual runtime vs integer size for large integers that are close together numerically:

```python, results='hidden'
plt.plot([r[0] for r in results], [r[1] for r in results])
plt.xlabel('Integer size (2ⁿ bit)')
plt.ylabel('Runtime (s)')
plt.title('Runtime impact of int size')
```

As an aside, the spike at 256 bit integers caught my attention. I haven't investigated this in detail, but I believe that this is likely due to running this on a CPU that doesn't support AVX2. AVX2 extensions provide support for native 256 bit math. I'm guessing that Python is hoping that this is available, but when it finds that it isn't it degrades to a code path that is actually less efficient than simply using its software bignum implementation.

For integers that are close together numerically, runtime cost increases as linearly with the size of the word, so we can't consider comparison to be a uniform operation in all cases.

However, on modern hardware a comparison between two hardware integers is a constant time operation. For example, this is a single cycle operation with single cycle latency on x86 CPUs[3]. Therefore when dealing with hardware integers, we can treat this as a uniform cost problem.

# 3.

> Demonstrate that some function/algorithm/code fragment is O() or \~
some other function. This would be similar to the problems in POA
3.3 or 3.4. You can choose the functions you want to work with and
they needn’t be the same as used in your other problems.

I will show that the above selection sort algorithm has complexity O(n²). Consider the basic algorithm is as follows:

1. Create an empty list of sorted items
2. Perform a comparison operation for every item in the unsorted list to determine the fittest item.
3. Move this item to the list of sorted items.
4. Return to step 2 until there are no more unsorted items.

For every item in the list, we must perform (n-1) comparisons where n is the remaining number of unsorted items.

This means that the number of comparisons is represented by the summation of (n-i) where i goes from 1 to n.

This summation can be rewritten as:

$\sum_{i=1}^{n} (n-1) = \frac{n(n+1)}{2}$

$\sum_{i=1}^{n} (n-1) = \frac{n^2+n}{2}$

$\sum_{i=1}^{n} (n-1) = \frac{1}{2}n^2+\frac{1}{2}n$

To prove that this algorithm is O(n²), we need to show that there exists a constant `c` such that:

$\frac{1}{2}n^2+\frac{1}{2}n \le cn^2$

We can simplify the above expression to:

$(\frac{1}{2}-c)n^2+\frac{1}{2}n \le 0$

Consider the case where c=1:

$(-\frac{1}{2})n^2+\frac{1}{2}n \le 0$

The roots of the left hand size are 0 and 1. When $n\ge1$, the n² term will overpower the linear term. This proves that the above expression is true for $n\ge1$ and that our algorithm is O(n²) by definition.

# 4.

> Set up and solve a recurrence relation that represents the
performance of a function (of your choice). There are examples of
this in Algs4, pp272, 293, 383; in section 4 of POA; in the
Algorithm Analysis 3 section of the LNOA; and in chapter 10 of Math
for CS.

Let's explore selection sort using a recurrence relation. Consider the following cases:

- Our list to be sorted is empty. In this case, we have to comparisons to complete, s T₁=0.
- If our list has items in it, we must complete n-1 comparisons, where n is the length of our list. Therefore, Tₙ=Tₙ₋₁+n-1.

Here's the recurrence for selection sort:

    T₁=0
    Tₙ=Tₙ₋₁+n-1

Let's try solving this simply using guess-and-verify:

    T₁=0
    T₂=T₁+n-1=1
    T₃=T₂+n-1=3
    T₄=T₃+n-1=6
    T₅=T₄+n-1=10

For the sake of this assignment, let's pretend I have no idea what that pattern is. Let's try exploring this problem using plug-and-chug:

    Tₙ=Tₙ₋₁+n-1
      =(Tₙ₋₂+n-2)+n-1
      =Tₙ₋₂+(n-2)+(n-1)
      =Tₙ₋₃+(n-3)+(n-2)+(n-1)

The pattern is clearly:

$Tₙ=\sum_{i=1}^{n}(n-i)$

As shown previously, this reduces to:

$\frac{n(n+1)}{2}$



> Your demonstrations should be your own work and should not be trivial
examples of the particular processes. They can be based on examples you
have seen before, but you should be able to create the solutions
entirely from memory, without reference to any sources while doing the
demonstrations.

References
==========

1. https://en.wikipedia.org/wiki/Selection_sort
2. https://en.wikipedia.org/wiki/Triangular_number
3. http://www.agner.org/optimize/instruction_tables.pdf
4. Parberry, Ian. Problems on algorithms. Dover Publications, Inc., 2007.